{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_MB_Merkblatt_Verwendung_von_generativer_KI_in_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:07,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_KI_bei_Leistungsnachweisen.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:01<00:04,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Bibliotheksangebot_Bachelorarbeit_HS24FS25.pdf'>\n",
      "<_io.BufferedReader name='data/ZHAW_Zitierleitfaden_DE.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:02<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_Anhang_Deklarationspflicht_KI_bei_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:02<00:01,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/05_Checkliste_Sprachliche_Formale_Ausarbeitung.pdf'>\n",
      "<_io.BufferedReader name='data/Schwerpunktthemen_fuer_Studenten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:03<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/02_Merkblatt_Vermeidung-von-Plagiaten_0916.pdf'>\n",
      "<_io.BufferedReader name='data/W_MB_Merkblatt_Bachelorarbeit_BSc.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.32it/s]\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Z-MB-Merkblatt Verwendung von  \\ngenerativer KI bei'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        print(file)\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter, in this case [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),  to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 62\n",
      "Preview of the first chunk: Z-MB-Merkblatt Verwendung von  \n",
      "generativer KI bei Arbeiten  \n",
      "Version:  1.2.0 gültig ab:  01.03.2025   Seite 1 von 5 \n",
      " Rektorat  \n",
      "Ressort Bildung  \n",
      "Verwendung von generativer KI bei Arbeiten  \n",
      "Dieses \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model (see [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html#sentencetransformerstokentextsplitter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('faiss'):\n",
    "    os.makedirs('faiss')\n",
    "    \n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"KI während der Bachelorarbeit\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Welche hilfsmittel sind erlaubt?'  # Replace with your actual query text if needed\n",
    "original_query_text = 'Wieviele Seiten muss die Arbeit sein?'  # Replace with your actual query text if needed\n",
    "\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model for RAG query encoding: Sahajtomar/German-semantic...\n",
      "RAG SentenceTransformer model loaded.\n",
      "Loading FAISS index from: faiss/faiss_index.index\n",
      "Loading text chunks from: faiss/chunks_mapping.pkl\n",
      "Successfully loaded FAISS index with 254 embeddings and 62 text chunks.\n",
      "Warning: Mismatch! FAISS index has 254 embeddings, but 62 text chunks were loaded.\n",
      "This will likely lead to errors or incorrect retrieval. Ensure the pickled file contains the correct text passages.\n",
      "Groq API key should have been loaded by the preceding cell (ID: 729a82ef).\n",
      "Warning: GROQ_API_KEY not found in environment variables. The LLM call will likely fail.\n",
      "Please ensure your .env file is correctly set up with 'GROQ_API_KEY=your_key' and the cell that loads it has been executed.\n",
      "RAG SentenceTransformer model loaded.\n",
      "Loading FAISS index from: faiss/faiss_index.index\n",
      "Loading text chunks from: faiss/chunks_mapping.pkl\n",
      "Successfully loaded FAISS index with 254 embeddings and 62 text chunks.\n",
      "Warning: Mismatch! FAISS index has 254 embeddings, but 62 text chunks were loaded.\n",
      "This will likely lead to errors or incorrect retrieval. Ensure the pickled file contains the correct text passages.\n",
      "Groq API key should have been loaded by the preceding cell (ID: 729a82ef).\n",
      "Warning: GROQ_API_KEY not found in environment variables. The LLM call will likely fail.\n",
      "Please ensure your .env file is correctly set up with 'GROQ_API_KEY=your_key' and the cell that loads it has been executed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "# load_dotenv is usually called once, but ensuring it here if this section is run independently.\n",
    "# from dotenv import load_dotenv \n",
    "import pickle # For loading the text chunks\n",
    "\n",
    "# --- Configuration for the RAG Task ---\n",
    "RAG_MODEL_NAME = \"Sahajtomar/German-semantic\"\n",
    "# Path to the FAISS index and chunks as created in the earlier parts of this notebook\n",
    "RAG_FAISS_INDEX_PATH = \"faiss/faiss_index.index\" \n",
    "RAG_TEXT_CHUNKS_PATH = \"faiss/chunks_mapping.pkl\" \n",
    "RAG_GROQ_LLM_MODEL = \"llama-3.3-70b-versatile\" # Specified in the task\n",
    "\n",
    "# --- Load Embedding Model for RAG ---\n",
    "# This model will be used to encode the user's query.\n",
    "# IMPORTANT: For effective semantic search, the FAISS index (RAG_FAISS_INDEX_PATH)\n",
    "# should have been built using embeddings from this *same* model.\n",
    "# If the index was built with a different model (e.g., \"paraphrase-multilingual-MiniLM-L12-v2\" \n",
    "# as potentially used in cells de78b3b6 & c7fce0c5), the retrieval quality may be suboptimal.\n",
    "print(f\"Loading SentenceTransformer model for RAG query encoding: {RAG_MODEL_NAME}...\")\n",
    "rag_embedding_model = SentenceTransformer(RAG_MODEL_NAME)\n",
    "print(\"RAG SentenceTransformer model loaded.\")\n",
    "\n",
    "# --- Load Pre-built FAISS Index and Text Chunks ---\n",
    "def load_rag_faiss_index_and_chunks():\n",
    "    if not os.path.exists(RAG_FAISS_INDEX_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"FAISS index file not found: '{RAG_FAISS_INDEX_PATH}'. \"\n",
    "            \"Please ensure the notebook cells that create and save this file (e.g., cell 3ba2a6dd) have been executed correctly.\"\n",
    "        )\n",
    "    if not os.path.exists(RAG_TEXT_CHUNKS_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Text chunks file not found: '{RAG_TEXT_CHUNKS_PATH}'. \"\n",
    "            \"This file should contain the list of text passages corresponding to the FAISS embeddings.\"\n",
    "        )\n",
    "        \n",
    "    print(f\"Loading FAISS index from: {RAG_FAISS_INDEX_PATH}\")\n",
    "    rag_faiss_index = faiss.read_index(RAG_FAISS_INDEX_PATH)\n",
    "    \n",
    "    print(f\"Loading text chunks from: {RAG_TEXT_CHUNKS_PATH}\")\n",
    "    with open(RAG_TEXT_CHUNKS_PATH, \"rb\") as f:\n",
    "        # This should be the list of text strings that were embedded and stored in the FAISS index.\n",
    "        # In this notebook, cell c7fce0c5 builds the index from 'chunk_embeddings' derived from 'token_split_texts'.\n",
    "        # Cell 3ba2a6dd saves 'chunks' to 'faiss/chunks_mapping.pkl'. This might be 'token_split_texts' or the larger 'chunks'.\n",
    "        # Ensure that what's loaded here matches the content whose embeddings are in the FAISS index.\n",
    "        rag_text_chunks = pickle.load(f)\n",
    "        \n",
    "    print(f\"Successfully loaded FAISS index with {rag_faiss_index.ntotal} embeddings and {len(rag_text_chunks)} text chunks.\")\n",
    "    \n",
    "    if rag_faiss_index.ntotal != len(rag_text_chunks):\n",
    "        print(f\"Warning: Mismatch! FAISS index has {rag_faiss_index.ntotal} embeddings, but {len(rag_text_chunks)} text chunks were loaded.\")\n",
    "        print(\"This will likely lead to errors or incorrect retrieval. Ensure the pickled file contains the correct text passages.\")\n",
    "        \n",
    "    return rag_faiss_index, rag_text_chunks\n",
    "\n",
    "# Attempt to load the data\n",
    "rag_faiss_index, rag_text_chunks = None, []\n",
    "try:\n",
    "    rag_faiss_index, rag_text_chunks = load_rag_faiss_index_and_chunks()\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading pre-built FAISS data: {e}\")\n",
    "    print(\"Please ensure the FAISS index and text chunks are correctly generated and saved by earlier notebook cells,\")\n",
    "    print(\"and that the text chunks correspond to the embeddings in the FAISS index (ideally 'token_split_texts' created with the same embedding model).\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading FAISS data: {e}\")\n",
    "\n",
    "# Groq API Key is expected to be loaded into environment by cell 729a82ef\n",
    "# We will initialize the Groq client later, just before making an API call.\n",
    "print(f\"Groq API key should have been loaded by the preceding cell (ID: 729a82ef).\")\n",
    "if not os.getenv(\"GROQ_API_KEY\"):\n",
    "    print(\"Warning: GROQ_API_KEY not found in environment variables. The LLM call will likely fail.\")\n",
    "    print(\"Please ensure your .env file is correctly set up with 'GROQ_API_KEY=your_key' and the cell that loads it has been executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Embedding-Based Semantic Retrieval Function ---\n",
    "def get_semantic_retrieval_task(query: str, model: SentenceTransformer, index: faiss.Index, text_chunks_list: list, k: int = 3):\n",
    "    \"\"\"\n",
    "    Encodes the user query using the provided model and performs a nearest-neighbor search \n",
    "    in the FAISS index to retrieve the top-k similar text chunks.\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        print(\"Error: FAISS index is not loaded. Cannot perform retrieval.\")\n",
    "        return []\n",
    "    if not text_chunks_list:\n",
    "        print(\"Error: Text chunks list is empty or not loaded. Cannot perform retrieval.\")\n",
    "        return []\n",
    "    if k > index.ntotal:\n",
    "        print(f\"Warning: Requested k={k} is greater than the number of items in FAISS index ({index.ntotal}). Will retrieve {index.ntotal} items instead.\")\n",
    "        k = index.ntotal\n",
    "        \n",
    "    print(f\"Encoding query for retrieval: \\\\\"{query}\\\\\"\")\n",
    "    query_embedding = model.encode([query]) # SentenceTransformer expects a list of sentences\n",
    "    \n",
    "    print(f\"Searching FAISS index for top {k} similar chunks...\")\n",
    "    distances, indices = index.search(query_embedding, k) # D, I\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for i in indices[0]:\n",
    "        if i < len(text_chunks_list):\n",
    "            retrieved_chunks.append(text_chunks_list[i])\n",
    "        else:\n",
    "            print(f\"Warning: Index {i} from FAISS search is out of bounds for the loaded text_chunks_list (length {len(text_chunks_list)}). Skipping.\")\n",
    "            \n",
    "    if len(retrieved_chunks) == 0 and k > 0:\n",
    "        print(\"Warning: No chunks were retrieved. This might be due to an issue with the FAISS index or text_chunks_list alignment.\")\n",
    "    else:\n",
    "        print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
    "    return retrieved_chunks\n",
    "\n",
    "# --- 2. LLM Prompt Construction and Query Answering Function ---\n",
    "def build_prompt_and_query_llm_task(query: str, retrieved_chunks: list, groq_llm_model_name: str):\n",
    "    \"\"\"\n",
    "    Builds a prompt using the retrieved text chunks as context and queries the Groq LLM API.\n",
    "    \"\"\"\n",
    "    if not retrieved_chunks:\n",
    "        print(\"No relevant text chunks were retrieved to provide context for the LLM. Answering without specific context or indicating unavailability.\")\n",
    "        # Decide behavior: either ask LLM without context, or return a message.\n",
    "        # For this task, we'll make the context optional in the prompt structure if empty.\n",
    "        context_block = \"Kein spezifischer Kontext aus den Dokumenten gefunden.\"\n",
    "    else:\n",
    "        context_block = \"\\\\n\\\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"Beantworte die folgende Frage. Nutze dafür vorrangig den bereitgestellten Kontext.\n",
    "Wenn die Antwort nicht eindeutig im Kontext enthalten ist, versuche die Frage allgemein zu beantworten oder gib an, dass die Informationen basierend auf dem Kontext nicht verfügbar sind.\n",
    "\n",
    "Kontext:\n",
    "---\n",
    "{context_block}\n",
    "---\n",
    "\n",
    "Frage: {query}\n",
    "\n",
    "Antwort:\n",
    "\"\"\"\n",
    "    print(\"\\\\n--- Generierter Prompt für LLM ---\")\n",
    "    print(prompt)\n",
    "    print(\"---------------------------------\\\\n\")\n",
    "\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: GROQ_API_KEY not found in environment variables. Cannot query LLM.\")\n",
    "        return \"Fehler: GROQ_API_KEY nicht konfiguriert.\"\n",
    "        \n",
    "    client = Groq(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        print(f\"Sending request to Groq LLM model: {groq_llm_model_name}...\")\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=groq_llm_model_name,\n",
    "            temperature=0.2, \n",
    "            max_tokens=1024, \n",
    "        )\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        print(\"Antwort vom LLM erhalten.\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Anfrage an die Groq API ({groq_llm_model_name}): {e}\")\n",
    "        # Example of trying a common fallback model. Check Groq documentation for available models.\n",
    "        fallback_model = \"llama3-8b-8192\" \n",
    "        if groq_llm_model_name != fallback_model:\n",
    "            print(f\"Versuche Fallback-Modell '{fallback_model}'...\")\n",
    "            try:\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=fallback_model,\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=1024\n",
    "                )\n",
    "                response = chat_completion.choices[0].message.content\n",
    "                print(f\"Antwort vom LLM ({fallback_model}) erhalten.\")\n",
    "                return response\n",
    "            except Exception as fallback_e:\n",
    "                print(f\"Fehler auch mit Fallback-Modell '{fallback_model}': {fallback_e}\")\n",
    "        return f\"Fehler bei der Kommunikation mit dem LLM: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. User Query Execution ---\n",
    "# Example query from the task description\n",
    "user_query_task = \"Was ist der wichtigste Faktor bei der Diagnose von Asthma?\"\n",
    "# Alternative query relevant to the PDF names in the 'data' directory (you can uncomment to try)\n",
    "# user_query_task = \"Welche Hilfsmittel sind bei einer Bachelorarbeit erlaubt?\" \n",
    "# user_query_task = \"Wie werden Plagiate vermieden?\"\n",
    "\n",
    "print(f\"\\\\nBenutzeranfrage für RAG-Pipeline: {user_query_task}\")\n",
    "\n",
    "# Ensure all components are loaded before proceeding\n",
    "if rag_faiss_index and rag_text_chunks and rag_embedding_model:\n",
    "    # 1. Embedding-based Semantic Retrieval\n",
    "    print(\"\\\\nSchritt 1: Starte semantisches Retrieval...\")\n",
    "    # Using k=3 to get a bit more context, can be adjusted.\n",
    "    retrieved_passages_task = get_semantic_retrieval_task(\n",
    "        user_query_task, \n",
    "        rag_embedding_model, \n",
    "        rag_faiss_index, \n",
    "        rag_text_chunks, \n",
    "        k=3 \n",
    "    )\n",
    "    \n",
    "    if retrieved_passages_task:\n",
    "        print(\"\\\\n--- Abgerufene Passagen (max. 200 Zeichen Vorschau) ---\")\n",
    "        for i, passage in enumerate(retrieved_passages_task):\n",
    "            print(f\"  {i+1}. {passage[:200]}...\")\n",
    "        print(\"-----------------------------------------------------\\\\n\")\n",
    "    else:\n",
    "        print(\"Keine Passagen für die Anfrage abgerufen. Das LLM wird ohne spezifischen Kontext antworten.\")\n",
    "\n",
    "    # 2. LLM Prompt Construction and Query Answering\n",
    "    print(\"\\\\nSchritt 2: Konstruiere Prompt und frage LLM an...\")\n",
    "    llm_answer_task = build_prompt_and_query_llm_task(\n",
    "        user_query_task, \n",
    "        retrieved_passages_task, \n",
    "        RAG_GROQ_LLM_MODEL\n",
    "    )\n",
    "\n",
    "    # 3. Print the final answer from the LLM\n",
    "    print(\"\\\\n--- Finale Antwort vom LLM ---\")\n",
    "    print(llm_answer_task)\n",
    "    print(\"-------------------------------\\\\n\")\n",
    "else:\n",
    "    print(\"\\\\nFEHLER: Die RAG-Pipeline kann nicht ausgeführt werden.\")\n",
    "    print(\"Eine oder mehrere erforderliche Komponenten (FAISS-Index, Text-Chunks oder Embedding-Modell) wurden nicht korrekt geladen.\")\n",
    "    print(\"Bitte überprüfen Sie die Ausgaben der vorherigen Zellen und stellen Sie sicher, dass alle Daten korrekt initialisiert wurden,\")\n",
    "    print(\"insbesondere die Pfade zu den FAISS-Dateien und die Kompatibilität des Embedding-Modells mit dem Index.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
